{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sales Forecasting by Product and Market "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Index\n",
    "\n",
    "<a href='#Overview'>Source Data'</a>\n",
    "- a. Selection criteria to run SAP BI extract\n",
    "- b. Layout of BI extract.\n",
    "- c. Overview of the data\n",
    "\n",
    "\n",
    "<a href='#Preprocessing'>Preprocessing'</a>\n",
    "- 1. Ensure the year is 4 digits long\n",
    "- 2. Ensure only the data for the relevant scope is kept; the rest is dropped\n",
    "- 3. Format the value columns into a float¶\n",
    "- 4. Identify entity and market to be analysed\n",
    "- 5. Identify the data's datetime and remove 2022\n",
    "- 6. Ensure all months, in the analysis windows, are included in the dataframe\n",
    "- 7. Fill missing YTD values\n",
    "- 8. Identify monthly movements¶\n",
    "- 9. Data Analysis of the whole TimeSeries\n",
    "        - a. Descriptive Statistics\n",
    "        - b. Visual Analysis - Original, YTD and Monthly Mov\n",
    "        - c. Visual analysis - monthly movements only\n",
    "- 10. Address outliers\n",
    "\n",
    "\n",
    "\n",
    "<a href='#Stationarity'>Stationarity'</a>\n",
    "- 11 . Serie Stationarity\n",
    "    - a. Decomposition\n",
    "    - b. Differencing to make the series stationary\n",
    "    - c. Testing stationarity (ADF test)\n",
    "    - d. Copies of df for alternative modelling approaches\n",
    "    - e. Identify train and validation set\n",
    "\n",
    "<a href='#Parameters'>Arima's parameters'</a>\n",
    "- 21. ARIMA approach (over all time series) - Find p & q parameters' values\n",
    "        - a. PACF\n",
    "        - b. ACF\n",
    "        - c. Find the values of p and q for the ARIMA models\n",
    "        - d . Analysis of residuals from the AR model\n",
    "\n",
    "<a href='#Arima'>ARIMA's models'</a>\n",
    "- 22. Forecasting AR, ARMA, SARIMAX Models\n",
    "        - a. Identify train and validation set\n",
    "        - b. Fit the data\n",
    "            - Model no 1 - AR model\n",
    "            - Model no 2 - ARIMA model (taken from ARIMA class where d=0)\n",
    "            - Model no 3 - SARIMAX Model\n",
    "        - c. Analysis of residual\n",
    "\n",
    "\n",
    "<a href='#Prophet'>Prophet Model</a>\n",
    "- 23. Forecasting Prophet Model \n",
    "\n",
    "<a href='#NN'>Neural Network Model'</a>\n",
    "- 24. Neural Network\n",
    "\n",
    "<a href='#XG'>XGBoost Model'</a>\n",
    "- 25. Forecasting XGBoost\n",
    "\n",
    "<a href='#HW'>Holt-Wilson Model'</a>\n",
    "- 26. Forecasting Holt-Wilson Mode\n",
    "\n",
    "<a href='#Darts'>Darts Model'</a>\n",
    "- 27. Forecasting Darts Model\n",
    "\n",
    "<a href='#MAE'>Metrics'</a>\n",
    "- 30. Models' MAE comparison\n",
    "- 31. Models' MAPE comparison\n",
    "- 32. Models' RSME comparison\n",
    "- 33. Models' MASE comparison\n",
    "- 34. Metrics Summary\n",
    "- 35. Selection of best ML model\n",
    "- 36. Notes about Metrics\n",
    "\n",
    "<a href='#FOR'>Bring forecasts back to original scale'</a>\n",
    "- 40. Monthly Forecasts\n",
    "- 41. YTD Forecasts\n",
    "\n",
    "<a href='#Hyper'>Appendix  - Sarimax Model optimization '</a>\n",
    "- 50. HyperParameters Optimization (with fixed train/ test split)\n",
    "- 51. Cross Validation (with fixed order and seasonal order)\n",
    "- 52. Cross Validation and HyperParameters Optimization (with variable train/ test split and variable order and seasonal order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\dask\\dataframe\\utils.py:15: FutureWarning: pandas.util.testing is deprecated. Use the functions in the public API at pandas.testing instead.\n",
      "  import pandas.util.testing as tm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from pandas.plotting import lag_plot\n",
    "from pandas.plotting import autocorrelation_plot\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "%matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import shapiro\n",
    "\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.metrics import mean_absolute_percentage_error\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sktime.performance_metrics.forecasting import MeanAbsoluteScaledError\n",
    "from statsmodels.api import tsa\n",
    "\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.ar_model import ar_select_order\n",
    "from statsmodels.tsa.seasonal import seasonal_decompose\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "from statsmodels.tsa.arima.model import ARIMA\n",
    "\n",
    "from fbprophet import Prophet\n",
    "from fbprophet.plot import add_changepoints_to_plot\n",
    "\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import LSTM\n",
    "\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "from statsmodels.tsa.holtwinters import SimpleExpSmoothing   \n",
    "from statsmodels.tsa.holtwinters import ExponentialSmoothing\n",
    "\n",
    "import darts\n",
    "from darts import TimeSeries\n",
    "from darts.models import ExponentialSmoothing as ES\n",
    "from darts.metrics import mape\n",
    "\n",
    "from typing import *\n",
    "from dateutil.relativedelta import relativedelta\n",
    "\n",
    "import itertools\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\n",
    "\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_start = time.time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format\n",
    "pd.set_option('display.max_rows', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('fbprophet').setLevel(logging.WARNING) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Overview'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Source Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### a.  Selection criteria to run SAP BI extract"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### b. Layout of BI extract."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load source file \n",
    "data = pd.read_csv('SalesFile.csv', skiprows=3, encoding= 'unicode_escape') \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rename columns\n",
    "data=data.rename(columns={'Account Row':'Acct','Unnamed: 1':'Acct_description','Company code':'Entity','Data Entry point':'DEP','Audit ID':'Au_Id','Unnamed: 3':'Product_Description', 'Unnamed: 5':'Mkt_Description','Unnamed: 8':'AU_Description','Unnamed: 10':'Entity_Description','Unnamed: 13':'Prod_Description','£':'Value','Year/Period':'YrPd'})\n",
    "data.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### c. Overview of the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Quick overview of dataset's data type \n",
    "data.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Preprocessing'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  1. Ensure the year is 4 digits long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# ensure the year is 4 digits long\n",
    "'''\n",
    "A: YrPd is converted into a string so that we can use RE to retrieve those entries with a 3 digits year. \n",
    "Once identified, a zero is added to these offensive years. Years in the correct format are left untouched.\n",
    "\n",
    "'''\n",
    "\n",
    "data['YrPd']=data['YrPd'].astype(str)\n",
    "y=r\"^(\\d*\\.\\d{3})$\"  \n",
    "def fix_year(data):\n",
    "    \"\"\" A function to ensure year is 4 digits long \"\"\"    \n",
    "    x=re.findall(y,str(data))\n",
    "    if x:\n",
    "        return str(data)+'0'\n",
    "    else:\n",
    "        return data\n",
    "    \n",
    "data['YrPd']=data['YrPd'].apply(fix_year)\n",
    "#data.tail(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Ensure only the data for the relevant scope is kept; the rest is dropped"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# keep only the relevant scope\n",
    "''' \n",
    "A: Only the data where the Scope matches the DEP will be kept. \n",
    "In details: Only data where 2 last 2 characters of the Scope match the last 2 digits of the year in the DEP will be kept. \n",
    "The rest will be removed.\n",
    "\n",
    "'''\n",
    "def final_scope(data):\n",
    "    '''A function retain data where the scope matches the year '''\n",
    "    if data.loc['YrPd'][-2:]== data.loc['Scope'][-2:]:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "data['Final_Scope']=data.apply(final_scope,axis=1)\n",
    "data=data[data['Final_Scope']==1]\n",
    "df=data.copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Features no longer needed are removed.\n",
    "df.drop(['Version','Rate','Au_Id','Scope','Final_Scope'],axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view information about the data in the dataset\n",
    "#df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view size (no of rowsa and number of columns) of the dataset\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. format the value column into a float "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  format the value column into a float\n",
    "'''\n",
    "T: The Value data needs to be formatted as a float\n",
    "'''\n",
    "df['Value']=df['Value'].str.replace(',','')\n",
    "df['Value']=df['Value'].str.strip(')').str.replace('\\(','-').astype(float)\n",
    "df['Value']=df['Value']*-1\n",
    "#df.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Identify entity and market to be analysed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find no of datapoints by products and entities \n",
    "subset=df.groupby(['Product','Entity']).agg(date_min=('DEP','min'), date_max=('DEP','max'),\n",
    "                                              size=('Product','size'), value_min=('Value','min'),\n",
    "                                              value_max=('Value','max')).sort_values(by=['size','value_max'],ascending=False)\n",
    "subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract the product and entity with the heighest no of datapoints.\n",
    "extract=subset.iloc[0,:]\n",
    "extract"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter only datapoints with the chose product and entity\n",
    "'''\n",
    "All values below should say 1 a part from:\n",
    "DEP, \n",
    "scope (no of years in consideration +1), \n",
    "YrPd (should be the same to DEP; if less it means there are null Value values) and \n",
    "Value \n",
    "'''\n",
    "prod=extract.name[0]\n",
    "ent=extract.name[1]\n",
    "noDpt=extract.size\n",
    "print( 'Product chosen: ',prod, '  Entity chosen: ', ent, '  Lowest DEP: ', \n",
    "      extract.date_min, '  Highest DEP: ', extract.date_max )\n",
    "df=df[(df['Product']==prod) & (df['Entity']==ent)]\n",
    "df.nunique()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product 499 = Dental hygiene related product"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Identify the data's datetime and remove 2022 data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set datetime index for the series\n",
    "'''\n",
    "A: Use the existing YrPd feature to create a list comprehension to retrieve the month and year and generate a datetime feature with this format: yyyy-mm-dd.\n",
    "Set this new feature as the index of the time series.\n",
    "\n",
    "'''\n",
    "df['Date'] = [datetime(day=1, month=int(s[0]), year=int(s[-4:]))  \n",
    "              if len(s)==6 else datetime(day=1,month=int(s[:2]), year=int(s[-4:]) ) \n",
    "              for s in df['YrPd']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove 2022 data from the dataset\n",
    "'''\n",
    "A: drop the first 6 months of 2022 from the dataset so we have data for full years\n",
    "'''\n",
    "df=df[df['Date']<='2021-12-01']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('Date', inplace=True)\n",
    "df=df['Value']\n",
    "# attach a data frequency to the date column\n",
    "df.index = pd.DatetimeIndex(df.index.values,\n",
    "                               freq=df.index.inferred_freq)\n",
    "df.name = 'Original_values'\n",
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Ensure all months, in the analysis windows, are included in the timeframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ensure all timespoints are in the series with no gaps\n",
    "''' \n",
    "T: Ensure the timeseries has all the datapoint expected according to the frequency of the data collection \n",
    "(All months and years, within the timeframe under analysis, need to be present in the dataframe's index) \n",
    "A: Generate a complete Timestamp object, for the duration of all the expected timeserie, and concatenate \n",
    "the complete Timestamp object with the timeseries to see where missing data is\n",
    "\n",
    "'''\n",
    "\n",
    "max_val=df.index.max()\n",
    "print(max_val)\n",
    "min_val=df.index.min()\n",
    "print(min_val)\n",
    "span1=len(pd.date_range(min_val,max_val,freq='M'))+1\n",
    "print(span1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a df with all the months included within the lowest and highest datastamp\n",
    "dti = pd.date_range(min_val, periods=span1, freq=\"MS\")\n",
    "dti=dti.to_series().to_frame()\n",
    "df=pd.concat([dti,df], axis=1)\n",
    "df=df.drop([0],axis=1)\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. fill missing YTD values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use backfill to fill all the YTD values missing in the series \n",
    "'''\n",
    "T: Add data where the values are Nan \n",
    "A: Create a copy of the existing dataframe where all NaN rows will be filled with calculated values based on the \n",
    "YTD position in December as a proportion based on the missing month and concatenate the complete Values column\n",
    "to the existing df for comparison. \n",
    "'''\n",
    "# check for null values\n",
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytd_values=df.copy()\n",
    "ytd_values=ytd_values[ytd_values.index.month==12]\n",
    "ytd_values.tail(4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nan_replacement = pd.date_range(min_val, periods=span1, freq=\"MS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nan_replacement=Nan_replacement.to_series().to_frame()\n",
    "Nan_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nan_replacement=pd.concat([Nan_replacement,ytd_values], axis=1)\n",
    "Nan_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nan_replacement=Nan_replacement.drop([0],axis=1)\n",
    "Nan_replacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Nan_replacement['Month']=Nan_replacement.index.month\n",
    "Nan_replacement['Year']=Nan_replacement.index.year\n",
    "Nan_replacement['Original_YTD_values']=Nan_replacement['Original_values'].fillna(method='bfill')\n",
    "Nan_replacement['NAN_replacement']=Nan_replacement['Original_YTD_values']/12*Nan_replacement['Month']\n",
    "Nan_replacement.tail(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test\n",
    "\n",
    "#df.loc['2021-09-01','Original_values']=np.nan\n",
    "df=pd.concat([df,Nan_replacement['NAN_replacement']], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add 1/2 value based on 'Filled_values' column, into Final_YTD column to estimate the YTD values missing in the oringal data\n",
    "def final_ytd(df):\n",
    "    \"\"\"A function to replace NAN values with a proportion of the final values\"\"\"\n",
    "    if pd.isna(df['Original_values']):#|df['Original_values']==0:\n",
    "        return df['NAN_replacement']\n",
    "    else:\n",
    "        return df['Original_values']\n",
    "        #return 1\n",
    "    \n",
    "df['Final_Ytd']=df.apply(final_ytd,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.tail(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. identify monthly movements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use YTD movements to find monthly movements\n",
    "'''\n",
    "\n",
    "T: Values need to be converted into monthly movements so that it can be used by the model\n",
    "A: Convert values from YTD to monthly movements. This is done by subtracting the value of the current observation \n",
    "against the value of the previous observation. \n",
    "This approach nevertheless causes a problem when we are removing December data from January data. To overcome this problem, \n",
    "the existing monthly movement, for January, is replaced by its YTD observation (either original or calculated)\n",
    "'''\n",
    "\n",
    "df_ytd=df.drop(['Original_values','NAN_replacement'],axis=1)\n",
    "df_diff=df_ytd.diff()\n",
    "df_diff=df_diff.rename(columns={'Final_Ytd':'Monthly_variance'})\n",
    "df_final=pd.concat([df,df_diff], axis=1)\n",
    "df_final['Month']=df_final.index.month\n",
    "\n",
    "# set the monthly value of Jan equal to the YTD value of Jan\n",
    "\n",
    "def final_value(df):\n",
    "    \"\"\"A function to calculate monthly values\"\"\"\n",
    "    if df['Month']==1 :\n",
    "        return df['Final_Ytd']\n",
    "    else:\n",
    "        return df['Monthly_variance']\n",
    "df_final['Monthly_values']=df_final.apply(final_value,axis=1)\n",
    "print('df len: ',len(df_final))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "last_monthly_value =df_final.iloc[len(df_final)-1,:]['Monthly_values']\n",
    "last_monthly_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final['Monthly_values'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Data Analysis of the whole TimeSeries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Visual Analysis - Original, YTD and Monthly Mov"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# visualize the data\n",
    "\n",
    "'''\n",
    "T:Ensure the timeseries is stationary\n",
    "A: Use all available tools (visual, statistical) to validate the time series is static\n",
    "'''\n",
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(df_final['Original_values'], '-o', label=\"YTD Actuals\")\n",
    "plt.plot(df_final['Final_Ytd'], '--*', label=\"YTD revised\")\n",
    "plt.plot(df_final['Monthly_values'], '-o', label=\"Monthly Movements\")\n",
    "plt.title(\"YTD Actuals, YTD revised, Monthly Movements £\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"£\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The monthly movements seem fairly stationary. More analysis is done further down"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove all features which are not needed for the model, to leave only monthly movements\n",
    "ts=df_final.copy()\n",
    "ts=ts.drop(['Original_values','NAN_replacement','Final_Ytd','Monthly_variance','Month'],axis=1)\n",
    "ts=ts.dropna(subset=['Monthly_values'])\n",
    "ts=ts.rename(columns={'Monthly_values':'Values'})\n",
    "ts.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_avg = ts.rolling(12).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b.  Visual analysis - monthly movements only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 8))\n",
    "plt.plot(ts['Values'], '-o', label=\"Monthly Movements\")\n",
    "plt.plot(moving_avg, color='red')\n",
    "plt.title(\" Monthly Movements £\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"£\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Descriptive Statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(ts)\n",
    "plt.title(\"Ts values distribution\")\n",
    "plt.xlabel('£')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify values' distribution: min, max, mean and std, and outlier.\n",
    "\n",
    "ts.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Test Normality\n",
    "'''\n",
    "Use the Shapiro test to assess normality.\n",
    "H0: The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution\n",
    "H1: The test rejects the hypothesis of normality when the p-value is less than or equal to 0.05\n",
    "'''\n",
    "\n",
    "df_shapiro = shapiro(ts)\n",
    "print('The c test p value is', df_shapiro.pvalue)\n",
    "if df_shapiro.pvalue > 0.05:\n",
    "    print('As the p-value from the Shapiro-test (', df_shapiro.pvalue,')>0.05, the residuals are drawn from a normal distribution')\n",
    "else:\n",
    "    print('As the p-value from the Shapiro-test (', df_shapiro.pvalue,')<=0.05, the residuals are NOT drawn from a normal distribution. This is not a problem as Normally distributed data is NOT required for Time Series analysis')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Address outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate summary statistics (mean and std deviation) and outliers' thresholds (beyond 3 std deviations)\n",
    "data_mean, data_std = mean(ts['Values']), std(ts['Values'])\n",
    "# identify outliers\n",
    "cut_off = data_std * 3\n",
    "lower, upper = data_mean - cut_off, data_mean + cut_off\n",
    "print('Mean:  ', data_mean, 'std:   ',data_std, 'Lower: ', lower, ' Upper: ', upper)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another way to retrieve statistical infornation\n",
    "\n",
    "ts.describe().loc['mean']['Values']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify outliers and replace them with the Values column mean; if nothing is returned, this means there are no outliers\n",
    "def outlier(df):\n",
    "    \"\"\"A function to calculate outliers\"\"\"\n",
    "    if df['Values']>upper  or df['Values']<lower:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "    \n",
    "ts['outlier']=ts.apply(outlier,axis=1)\n",
    "ts[ts['outlier']==1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace outliers value with the df mean\n",
    "df_mean=ts['Values'].mean()\n",
    "df_mean\n",
    "\n",
    "def update_df(df):\n",
    "    \"\"\"A function to relace outliers with mean values\"\"\"\n",
    "    if df['outlier']==1:\n",
    "        return df_mean   \n",
    "    else:\n",
    "        return df['Values']\n",
    "ts['Values']=ts.apply(update_df,axis=1)\n",
    "\n",
    "ts=ts.drop(columns=['outlier'])\n",
    "#ts[:12]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Stationarity'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11 . Serie Stationarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identify trend, seasonal and residual components of the series\n",
    "\n",
    "'''\n",
    "Decompose the series into Trend, Seasonality and Residuals\n",
    "'''\n",
    "\n",
    "decomposition = seasonal_decompose(ts)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "# Plot the decomposition\n",
    "\n",
    "plt.figure(figsize=(18, 6), dpi=80)\n",
    "plt.subplot(411)\n",
    "plt.plot(ts, label = 'Monthly Movements £')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label = 'Trend')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label = 'Seasonality')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label = 'Residuals')\n",
    "plt.legend(loc = 'best')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Result: the series is not stationary as the trend part of the plot shows a decreasing trend which needs to be addressed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. Differencing to make the series stationary "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_before_dif=ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# differencing the series to remove the trend and make it stationery\n",
    "ts=ts.diff(1).dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ts[-14:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the trend, seasonal and residual components again on the differentiated serie\n",
    "'''\n",
    "Decompose the differentiated series into Trend, Seasonality and Residuals\n",
    "'''\n",
    "\n",
    "decomposition = seasonal_decompose(ts)\n",
    "trend = decomposition.trend\n",
    "seasonal = decomposition.seasonal\n",
    "residual = decomposition.resid\n",
    "\n",
    "\n",
    "plt.figure(figsize=(18, 6), dpi=80)\n",
    "\n",
    "# Plot the results\n",
    "plt.subplot(411)\n",
    "plt.plot(ts, label = 'Actuals Monthly Movemements £')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(412)\n",
    "plt.plot(trend, label = 'Trend')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(413)\n",
    "plt.plot(seasonal, label = 'Seasonality')\n",
    "plt.legend(loc = 'best')\n",
    "plt.subplot(414)\n",
    "plt.plot(residual, label = 'Residuals')\n",
    "plt.legend(loc = 'best')\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The series now looks stationary as the trend does not show any increment or decrease over time.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Testing stationarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use ADF to test for stationarity\n",
    "\n",
    "'''\n",
    "If result at 5%  > ADF result  AND P-value <= (0.05),'the series is stationary')\n",
    "'''\n",
    "\n",
    "\n",
    "result = adfuller(ts, autolag='AIC')\n",
    "print('ADF Statistic: %f' % result[0])\n",
    "print('p-value: %.9f' % result[1])\n",
    "print(f'n_lags:{result[2]}')\n",
    "print('Critical Values:')\n",
    "for key, value in result[4].items():\n",
    "    print('\\t%s: %.3f' % (key, value))\n",
    "    \n",
    "if (result[1] <= 0.05) & (result[4]['5%'] > result[0]):\n",
    "    #print(\"\\u001b[32mTime Series is Stationary\\u001b[0m\")\n",
    "    print ('If result at 5% (',result[4]['5%'], ') > ADF result (', result[0], ') AND P-value:(%.9f '%result[1],') <= (0.05)','the series is stationary')\n",
    "else:\n",
    "    print(\"\\x1b[31mTime Series is Non-stationary\\x1b[0m\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### d. Copies of df for alternative modelling approaches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_copy=ts.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# due to differencing, the first row of the TS is dropped. \n",
    "ts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# future forecasting dates to use by models:\n",
    "\n",
    "date_after_month = datetime.today()+ relativedelta(months=1)\n",
    "future=pd.date_range(max_val+ relativedelta(months=0),max_val+ relativedelta(months=12),freq='MS')\n",
    "future"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### e.   Identify train and validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the time series is now split into 2 parts:\n",
    "'''\n",
    "train set (also called “in-sample data”): the size of this set should be between 70% to 80% of the total dataset\n",
    "test set (also called “hold-out set” or  “out-of-sample data” because these data are “held out” of the data used for fitting)\n",
    "'''\n",
    "\n",
    "train_test_split_per=0.7\n",
    "l_train=int(len(ts)*train_test_split_per) # find out the data point where the train set stops\n",
    "l_train=48 # use this if you want a constant training set size\n",
    "\n",
    "l_test= len(ts)-l_train # find out the data point where the test set starts\n",
    "\n",
    "#print(len(ts),l_train, l_test)\n",
    "train =  ts[:-l_test] # identify the train set\n",
    "test =  ts[l_train:] # identify the test set\n",
    "\n",
    "print('Train: ',train.shape, 'Test: ', test.shape, 'Ts: ', ts.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Parameters'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Arima's parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 21. ARIMA model family approach (overview of the model over the whole time series) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a. PACF (to find value of p for AR() model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the PACF function to identify auto correlation of data with previous values\n",
    "N, M = 12, 6\n",
    "fig, ax = plt.subplots(figsize=(N, M))\n",
    "plot_pacf(train['Values'], lags = 10, title='PACF plot', ax=ax, method='ywmle')\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel(\"PACF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b. ACF (to find value of q for MA() model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the ACF function to identify auto correlation of data with previous values\n",
    "N, M = 12, 6\n",
    "fig, ax = plt.subplots(figsize=(N, M))\n",
    "plot_acf(train['Values'], lags = 10, title='ACF plot', ax=ax)\n",
    "#plt.xlabel('Lags')\n",
    "#plt.ylabel(\"ACF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### c. Find the values of p for the ARIMA models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Identify the best value for p which tells how many timepoints (=time lag) \n",
    "should be taken into consideration for the regression.\n",
    "'''\n",
    "\n",
    "# identify optimal lag\n",
    "model = ar_select_order(train, maxlag=10, ic=\"aic\")\n",
    "optlag = max(model.ar_lags)\n",
    "print('Optimal p (with AIC Information Criterion) =', optlag)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p=9 is confirmed to be the best value to use with AR model, according to ar_select_order method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# determine the  best values of p (from the list made of PACT and ar_slect_order's findings)\n",
    "plt.figure(figsize=(12, 12))\n",
    "\n",
    "ar_orders=[1,2,5,8,9] # based on AR_select_order and the PACF graphs\n",
    "fitted_model_dict={}\n",
    "aic_results=[]\n",
    "aic_p_number=[]\n",
    "\n",
    "for idx, ar_order in enumerate(ar_orders):\n",
    "    ar_model = ARIMA(train, order=(ar_order,0,0))\n",
    "    ar_model_fit = ar_model.fit()\n",
    "    fitted_model_dict[ar_order]=ar_model_fit\n",
    "\n",
    "    \n",
    "    plt.subplot(5,1,idx+1)\n",
    "    plt.plot(train)\n",
    "    plt.plot(ar_model_fit.fittedvalues)\n",
    "    plt.title('AR%s fit' %ar_order,fontsize=16)\n",
    "    aic_results.append(fitted_model_dict[ar_order].aic)\n",
    "    aic_p_number.append(ar_order)\n",
    "    print('AIC for AR(%s): %s'%(ar_order, fitted_model_dict[ar_order].aic),\n",
    "          ' - BIC for AR(%s): %s'%(ar_order, fitted_model_dict[ar_order].bic))\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(aic_p_number, aic_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aic_result=pd.DataFrame([aic_p_number, aic_results],index=['aic_p_number','aic_results']).T\n",
    "aic_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_aic_result=aic_result[aic_result['aic_results']==aic_result['aic_results'].min()]\n",
    "print(best_aic_result)\n",
    "best_aic_result['aic_p_number']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Arima'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ARIMA's Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 22. Forecasting  AR, ARMA, SARIMAX Models \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model no 1 - AR model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an AR model on test data data \n",
    "'''\n",
    "At present the attention will be on the ARIMA (AutoRegressive (AR) Integrated (I) MovingAverage(MA)) models' family:\n",
    "\n",
    "AR (autoregressive) model: where regression is used on a given number of timepoints (p), \n",
    "which capture the behaviour of the series, to predict the next values, as the the assumption \n",
    "is that past values can be used to predict the next values\n",
    "'''\n",
    "\n",
    "# identify the best parameter for p\n",
    "mod_ar = ar_select_order(train, maxlag=12, ic=\"aic\")\n",
    "optlag = max(mod_ar.ar_lags)\n",
    "#optlag=1\n",
    "print('Optimal lag p : ',optlag)\n",
    "\n",
    "# instantiate the AR model\n",
    "ar = tsa.AutoReg(train, lags=optlag)\n",
    "\n",
    "# fit the AR model on train data\n",
    "arfit = ar.fit()\n",
    "\n",
    " # get the lenght of test data, identify lenght of the test data\n",
    "# make predictions on unseen data:\n",
    "prediction_ar = arfit.predict(end=ts.index[-1])[-len(test):]\n",
    "\n",
    "\n",
    "# Print MAE \n",
    "MAE_AR=mean_absolute_error(test, prediction_ar)\n",
    "print(\"AR's MAE on test data = {0:.3f}\".format(MAE_AR))\n",
    "\n",
    "# Print MAPE \n",
    "MAPE_AR=mean_absolute_percentage_error(test, prediction_ar)\n",
    "print(\"AR's MAPE on test data = {0:.3f}\".format(MAPE_AR))\n",
    "\n",
    "# Print RMSE\n",
    "RMSE_AR=mean_squared_error(test, prediction_ar, squared=False)**0.5\n",
    "print(\"AR's RMSE on test data = {0:.3f}\".format(RMSE_AR))\n",
    "\n",
    "# Print MASE\n",
    "MASE_AR=MeanAbsoluteScaledError()\n",
    "MASE_AR=MASE_AR(test, prediction_ar, y_train=train)\n",
    "print(\"AR's MASE on test data = {0:.3f}\".format(MASE_AR))\n",
    "\n",
    "# plot the forecasts against the true values\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts, '-o', label=\"Actuals Monthly Movemements\")\n",
    "plt.plot(prediction_ar, '--o', label='Prediction')\n",
    "plt.title(\"AR Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict into the future\n",
    "future_prediction_ar = arfit.predict(end=future[-1])[-len(future):]\n",
    "future_prediction_ar\n",
    "\n",
    "# plot the forecasts against the future\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts, '-o', label=\"Actuals Monthly Movemements\")\n",
    "plt.plot(prediction_ar, '--o', label='Prediction')\n",
    "plt.plot(future_prediction_ar, '--x', label='Future Predictions')\n",
    "plt.title(\"AR Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed future  predictions\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(test, '-o', label=\"test data\")\n",
    "plt.plot(prediction_ar, '--o', label='prediction')\n",
    "plt.plot(future_prediction_ar, '--x', label='prediction')\n",
    "plt.title(\"AR Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model no 2 - ARIMA model (taken from ARIMA class where d=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit an ARIMA model on test data data \n",
    "\n",
    "'''\n",
    "MA model: where the moving average (MA) of lagged past errors (q) is added to the AR model to account for different behaviours\n",
    "ARIMA model: which is able to process non-stationary data by including the integrated parameter (d), which counts the numbers of differentiations until stationarity is reached, to underlying ARMA model.\n",
    "\n",
    "  ARIMA model therefore requires not only stationarity, but also the identification of the value of 3 parameters called p, d and q.\n",
    "\n",
    "  - p (relevant for AR)is the number of autoregressive terms , \n",
    "  - d (relevant for I) is the number of nonseasonal differences needed for stationarity, and \n",
    "  - q (relevant for MA) is the number of lagged forecast errors in the prediction equation.\n",
    "'''\n",
    "\n",
    "# The timeseries has already be differentiated so d=0 as no additional differentiations are neeeded.\n",
    "\n",
    "# identify the best parameter for p and q \n",
    "param_choice = tsa.arma_order_select_ic(train, max_ar=12, max_ma=4, ic='aic', trend='c')\n",
    "print('Best of option for ARMA(p,q) =',param_choice['aic_min_order'])\n",
    "\n",
    "par_p=param_choice['aic_min_order'][0]\n",
    "par_q=param_choice['aic_min_order'][1]\n",
    "\n",
    "# as data is stationary, d=0 as the model does not need a further differentiation\n",
    "arima = sm.tsa.ARIMA(train, order=(par_p,0, par_q))\n",
    "arima_fit = arima.fit()\n",
    "\n",
    "# we make predictions on unseen data: \n",
    "prediction_ari = arima_fit.predict(end=ts.index[-1])[-len(test):]\n",
    "#print(prediction_ari)\n",
    "\n",
    "# Print MAE \n",
    "MAE_ARIMA = mean_absolute_error(test, prediction_ari)\n",
    "print(\"ARIMA's MAE = {0:.3f}\".format(MAE_ARIMA))\n",
    "\n",
    "# Print MAPE \n",
    "MAPE_ARIMA = mean_absolute_percentage_error(test, prediction_ari)\n",
    "print(\"ARIMA's MAPE = {0:.3f}\".format(MAPE_ARIMA))\n",
    "\n",
    "# Print RMSE\n",
    "RMSE_ARIMA=mean_squared_error(test, prediction_ari, squared=False)**0.5\n",
    "print(\"ARIMA's RMSE on test data = {0:.3f}\".format(RMSE_ARIMA))\n",
    "\n",
    "# Print MASE\n",
    "MASE_ARIMA=MeanAbsoluteScaledError()\n",
    "MASE_ARIMA=MASE_ARIMA(test, prediction_ari, y_train=train)\n",
    "print(\"ARIMA's MASE on test data = {0:.3f}\".format(MASE_ARIMA))\n",
    "\n",
    "# plot predictions v true data\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts, '-o', label=\"Actuals Monthly Movemements\")\n",
    "plt.plot(prediction_ari, '--o', label='Prediction')\n",
    "plt.title(\"ARIMA Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict into the future\n",
    "future_prediction_arima = arima_fit.predict(end=future[-1])[-len(future):]\n",
    "future_prediction_arima\n",
    "\n",
    "# plot the forecasts against the future\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts, '-o', label=\"Actuals Monthly Movemements\")\n",
    "plt.plot(prediction_ari, '--o', label='Prediction')\n",
    "plt.plot(future_prediction_arima, '--x', label='future predictions')\n",
    "plt.title(\"ARIMA Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed future ARIMA predictions\n",
    "\n",
    "plt.figure(figsize=(16,6))\n",
    "plt.plot(test, '-o', label=\"test data\")\n",
    "plt.plot(prediction_ari, '--o', label='prediction')\n",
    "plt.plot(future_prediction_arima, '--x', label='prediction')\n",
    "plt.title(\"ARIMA Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model no 3 - Sarimax Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize Sarimax model with the best hyperparameter identified in the search at the end of the Jupiter Book\n",
    "\n",
    "'''\n",
    "On top of the ARIMA's model, the SARIMAX (Seasonal Auto-Regressive Integrated Moving Average with \n",
    "eXogenous factors) model accounts also for seasonality. The notation for a SARIMAX model is specified as:SARIMA(p,d,q)(P,D,Q,m)\n",
    "\n",
    "    Configuring a SARIMA requires therefore selecting hyperparameters for both the trend (p,d,q), \n",
    "    mentioned above, and seasonal elements of the series (P,D,Q,m) which mean:.\n",
    "\n",
    "    - P: Seasonal autoregressive order.\n",
    "    - D: Seasonal difference order.\n",
    "    - Q: Seasonal moving average order.\n",
    "    - m: The number of time steps for a single seasonal period.\n",
    "'''\n",
    "# mod_sar = SARIMAX(train, trend='c', # use this if train set size = 70%\n",
    "#               order=(3,0,1), \n",
    "#               seasonal_order=(2,0,2,12),\n",
    "#               enforce_stationarity=False, \n",
    "#               enforce_invertibility=False)\n",
    "\n",
    "mod_sar = SARIMAX(train, trend='c', # use this when train set size = 48 \n",
    "              order=(1,0,3), \n",
    "              seasonal_order=(3,0,3,12),\n",
    "              enforce_stationarity=False, \n",
    "              enforce_invertibility=False)\n",
    "#fit model\n",
    "sarima_fit= mod_sar.fit()\n",
    "\n",
    "# run prediction\n",
    "prediction_sar = sarima_fit.predict(end=ts.index[-1])[-len(test):]\n",
    "\n",
    "# calculate MAE\n",
    "MAE_SARIMAX=mean_absolute_error(test, prediction_sar)\n",
    "print(\"SARIMAX' s MAE = {0:.3f}\".format(MAE_SARIMAX))\n",
    "\n",
    "# calculate MAPE\n",
    "MAPE_SARIMAX=mean_absolute_percentage_error(test, prediction_sar)\n",
    "print(\"SARIMAX' s MAPE = {0:.3f}\".format(MAPE_SARIMAX))\n",
    "\n",
    "# Print RMSE\n",
    "RMSE_SARIMAX=mean_squared_error(test, prediction_sar, squared=False)**0.5\n",
    "print(\"SARIMAX's RMSE on test data = {0:.3f}\".format(RMSE_SARIMAX))\n",
    "\n",
    "# Print MASE\n",
    "MASE_SARIMAX=MeanAbsoluteScaledError()\n",
    "MASE_SARIMAX=MASE_SARIMAX(test, prediction_sar, y_train=train)\n",
    "print(\"SARIMAX's MASE on test data = {0:.3f}\".format(MASE_SARIMAX))\n",
    "\n",
    "# plot predictions v true data\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts, '-o', label='true')\n",
    "plt.plot(prediction_sar, '-o', label='model')\n",
    "plt.title(\"SARIMAX Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend();\n",
    "plt.show()\n",
    "#print(test)\n",
    "#print('-'*20)\n",
    "#print(prediction_x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict into the future\n",
    "future_prediction_sar = sarima_fit.predict(end=future[-1])[-len(future):]\n",
    "future_prediction_sar\n",
    "\n",
    "# plot the forecasts against the future\n",
    "plt.figure(figsize=(16, 8))\n",
    "plt.plot(ts, '-o', label=\"Actuals Monthly Movemements\")\n",
    "plt.plot(prediction_sar, '--o', label='Prediction')\n",
    "plt.plot(future_prediction_sar, '--x', label='Future Predictions')\n",
    "plt.title(\"SARIMAX Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# zoomed FUTURE predictions\n",
    "plt.figure(figsize=(18, 6))\n",
    "plt.plot(test, '-o', label=\"test data\")\n",
    "plt.plot(prediction_sar, '--o', label='prediction')\n",
    "plt.plot(future_prediction_sar, '--x', label='prediction')\n",
    "plt.title(\"SARIMAX Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# identify residuals (=  difference between the true values (starting at the position of the optimal lag) and the predictions)\n",
    "plt.figure(figsize=(12, 8))\n",
    "fit_residuals = test['Values'] - prediction_sar \n",
    "#res_moving_avg = fit_residuals.rolling(12).mean()\n",
    "\n",
    "# Plot the residuals\n",
    "plt.plot(fit_residuals, '-o', label=\"Residuals\")\n",
    "#plt.plot(res_moving_avg, color='red')\n",
    "plt.title('Residuals', fontsize=12)\n",
    "plt.legend(fontsize=12);\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the histogram shows values are around the mean of zero with a std deviation between -400 and 400 with a few outlier \n",
    "plt.hist(fit_residuals, bins=8)\n",
    "plt.title(\"Residuals Distribution \")\n",
    "plt.xlabel('£')\n",
    "plt.ylabel(\"Frequency\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fit_residuals.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print ('Mean of residuals:%.9f '%fit_residuals.describe()[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N, M = 12, 6\n",
    "fig, ax = plt.subplots(figsize=(N, M))\n",
    "plot_acf(fit_residuals, lags = 20, title='ACF plot', ax=ax)\n",
    "plt.xlabel('Lags')\n",
    "plt.ylabel(\"ACF\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use the Shapiro test to assess normality.\n",
    "# H0: The Shapiro-Wilk test tests the null hypothesis that the data was drawn from a normal distribution\n",
    "#H1: The test rejects the hypothesis of normality when the p-value is less than or equal to 0.05\n",
    "\n",
    "test_shapiro = shapiro(fit_residuals)\n",
    "print('The Shapiro-Wilk test p value is', test_shapiro.pvalue)\n",
    "if test_shapiro.pvalue > 0.05:\n",
    "    print('As the p-value from the Shapiro-test (', test_shapiro.pvalue,')>0.05, the residuals are drawn from a normal distribution')\n",
    "else:\n",
    "    print('As the p-value from the Shapiro-test (', test_shapiro.pvalue,')<=0.05, the residuals are NOT drawn from a normal distribution')\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Prophet'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------------------------ Additional models    ------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 23. Forecasting  Prophet Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pr=ts_before_dif.copy()      #ts_copy.copy() # copy for prophet modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns to comply to Prophet's requirements\n",
    "'''\n",
    "The Prophet library is library designed for making forecasts for univariate time series datasets \n",
    "where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. \n",
    "\n",
    "Therefore, it is not necessary to differentiate the series\n",
    "\n",
    "This is a additive regression model which considers 3 elements:\n",
    "\n",
    "𝑦(𝑡)=𝑔(𝑡)+𝑠(𝑡)+ℎ(𝑡)+𝜖𝑡\n",
    "'''\n",
    "pr.reset_index(level=0, inplace=True)\n",
    "pr.rename(columns={'index':'ds', 'Values':'y'},inplace=True)\n",
    "pr.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_future_dates=future.to_frame().reset_index().drop(0, axis=1).rename({'index':'ds'})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_pr =  pr[:-l_test] # identify the train set\n",
    "test_pr =  pr[l_train:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_pr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# instantiate model and fit the data\n",
    "forecast_model = Prophet()   #Prophet(growth='linear', daily_seasonality=False)\n",
    "forecast_model_pr=forecast_model.fit(train_pr)\n",
    "pr_pred=forecast_model_pr.predict(test_pr)\n",
    "\n",
    "# Calculate and print MAE\n",
    "MAE_PROPHET=mean_absolute_error(test_pr.loc[:,'y'],pr_pred[:ts.shape[0]]['trend'])\n",
    "print(\"Prophet's MAE = {0:.3f}\".format(MAE_PROPHET))\n",
    "\n",
    "# Calculate and print MAPE\n",
    "MAPE_PROPHET=mean_absolute_percentage_error(test_pr.loc[:,'y'],pr_pred[:ts.shape[0]]['trend'])\n",
    "print(\"Prophet's MAPE = {0:.3f}\".format(MAPE_PROPHET))\n",
    "\n",
    "# Calculate and print RMSE\n",
    "RMSE_PROPHET=mean_squared_error(test_pr.loc[:,'y'],pr_pred[:ts.shape[0]]['trend'])**0.5\n",
    "print(\"Prophet's RMSE = {0:.3f}\".format(RMSE_PROPHET))\n",
    "\n",
    "# Calculate and print MASE\n",
    "MASE_PROPHET=MeanAbsoluteScaledError()\n",
    "MASE_PROPHET=MASE_PROPHET(test_pr.loc[:,'y'], pr_pred[:ts.shape[0]]['trend'], y_train=train_pr.loc[:,'y'])\n",
    "print(\"Prophet's MASE = {0:.3f}\".format(MASE_PROPHET))\n",
    "\n",
    "#pr_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# work out the dates the model will use\n",
    "df_dates = forecast_model_pr.make_future_dataframe(periods=12, \n",
    "                                                freq='M',\n",
    "                                                include_history=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate predictions (they are stored in a dataframe with yhat, trend, additive, yearly, multiplicative values )\n",
    "model_predictions_pr = forecast_model_pr.predict(df_dates)\n",
    "#model_predictions_future = forecast_model.predict(df_future_dates)\n",
    "\n",
    "#retrieve the trend data from the dataframe\n",
    "pr_pred=pd.DataFrame(model_predictions_pr['trend'])\n",
    "#pr_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Plot timeseries and forecasts\n",
    "plt.figure(figsize=(12, 8))\n",
    "#plot_pred = forecast_model_pr.plot(model_predictions_pr)\n",
    "fig = forecast_model_pr.plot(model_predictions_pr)\n",
    "\n",
    "\n",
    "changepoint_plot = add_changepoints_to_plot(fig.gca(), forecast_model_pr, model_predictions_pr)\n",
    "\n",
    "plt.title(\"Prophet Model\")\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.show();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_components = forecast_model_pr.plot_components(model_predictions_pr, uncertainty=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='NN'></a>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 24.  Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = ts_copy.copy() # copy for neural network modelling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a copy ts timeseries and keep only the values column\n",
    "'''\n",
    "eneral characteristics of LSTM's RNN:\n",
    "\n",
    "1 - In the RNN context, dates are not important \n",
    "as the timing of each observation is constant (monthly) \n",
    "\n",
    "2 - LSTMs are sensitive to the scale of the input data, and therefore the data needs to be \n",
    "normalized (rescaled in a 0 to 1 range)\n",
    "\n",
    "3 - The LSTM network expects the input data (X) to be provided with a specific \n",
    "array structure in the form of: [samples, time steps, features].\n",
    "'''\n",
    "\n",
    "nn_df=nn\n",
    "#nn_df.info()\n",
    "dataset = nn_df.values\n",
    "\n",
    "# ensure the new dataset has float32 format \n",
    "# note: dataset shape is the same as ts (= no yrs analysed x 12 (no months) - 1 (for differencing)) with 1 column only (values)\n",
    "dataset = dataset.astype('float32')\n",
    "print(dataset.shape)\n",
    "print( '6 years worth of monthly data \"2016 to 2021\" (72 data points) reduced by one due to differencing (now 71)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to transform the df into a matrix\n",
    "def create_dataset(dataset, look_back=1):\n",
    "    \"\"\"function to transform the df into a matrix\"\"\"\n",
    "    dataX, dataY = [], []\n",
    "    for i in range(len(dataset)-look_back-1): \n",
    "        a = dataset[i:(i+look_back), 0] # create matrix  (dataX) with moving starting position and constant width \n",
    "        dataX.append(a)\n",
    "        dataY.append(dataset[i + look_back, 0])# create array (dataY) having the following datapoint (on the right) of dataX\n",
    "    return np.array(dataX), np.array(dataY)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(11)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize the original dataset\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))\n",
    "dataset = scaler.fit_transform(dataset)\n",
    "\n",
    "# split the original dataset into train and test sets\n",
    "train_size = l_train #int(len(dataset) * train_test_split_per)\n",
    "test_size = l_test #len(dataset) - train_size\n",
    "train_NN = dataset[0:train_size,:]\n",
    "test_NN =  dataset[train_size:len(dataset),:]\n",
    "print('Train size: ', train_size, 'Test size: ', test_size, 'Ts size: ', len(ts))\n",
    "\n",
    "# reshape into X=t and Y=t+1 --> shape will be (m x n) \n",
    "look_back = 3\n",
    "trainX, trainY = create_dataset(train_NN, look_back)\n",
    "testX, testY = create_dataset(test_NN, look_back)\n",
    "print('Reshaped Train size: ', len(trainX), 'Reshaped Test size: ', len(testX))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshape input to be [samples, time steps, features] --> shape will be (m x n x 1)\n",
    "trainX = np.reshape(trainX, (trainX.shape[0], trainX.shape[1], 1))\n",
    "testX = np.reshape(testX, (testX.shape[0], testX.shape[1], 1))\n",
    "trainX.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create and fit the LSTM network\n",
    "\n",
    "# define batch size\n",
    "batch_size = 1\n",
    "\n",
    "# initiate sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# add layers, dense layer and optimizer to the model\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True, return_sequences=True))\n",
    "model.add(LSTM(4, batch_input_shape=(batch_size, look_back, 1), stateful=True))\n",
    "model.add(Dense(1))\n",
    "model.compile(loss='mean_squared_error', optimizer='adam')\n",
    "\n",
    "# fit data into the model to optimize the output\n",
    "for i in range(100):\n",
    "    model.fit(trainX, trainY, epochs=1, batch_size=batch_size, verbose=2, shuffle=False)\n",
    "    model.reset_states()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# make predictions using optimized model\n",
    "trainPredict = model.predict(trainX, batch_size=batch_size)\n",
    "model.reset_states()\n",
    "testPredict = model.predict(testX, batch_size=batch_size)\n",
    "testPredict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert predictions from a scaler value to its original value\n",
    "trainPredict = scaler.inverse_transform(trainPredict)\n",
    "trainY = scaler.inverse_transform([trainY])\n",
    "\n",
    "testPredict = scaler.inverse_transform(testPredict)\n",
    "testY = scaler.inverse_transform([testY])\n",
    "print(testY.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "testY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# invert train set from a scaler value to its original value\n",
    "# trainX = scaler.inverse_transform(trainX[0])\n",
    "# trainX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate MAE\n",
    "MAE_NN=mean_absolute_error(testY[0], testPredict[:,0])\n",
    "print(\"NN's MAE = {0:.3f}\".format(MAE_NN))\n",
    "\n",
    "# Calculate MAPE\n",
    "MAPE_NN=mean_absolute_percentage_error(testY[0], testPredict[:,0])\n",
    "print(\"NN's MAPE = {0:.3f}\".format(MAPE_NN))\n",
    "\n",
    "# Calculate RMSE\n",
    "RMSE_NN=mean_squared_error(testY[0], testPredict[:,0])**0.5\n",
    "print(\"NN's RMSE = {0:.3f}\".format(RMSE_NN))\n",
    "\n",
    "\n",
    "# Calculate MASE\n",
    "MASE_NN=MeanAbsoluteScaledError()\n",
    "MASE_NN=MASE_NN(testY[0],testPredict[:,0], y_train=trainX[0])\n",
    "print(\"NN's MASE = {0:.3f}\".format(MASE_NN))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the graph\n",
    "plt.figure(figsize=(12,8))\n",
    "# shift train predictions for plotting\n",
    "trainPredictPlot = np.empty_like(dataset)\n",
    "trainPredictPlot[:, :] = np.nan\n",
    "trainPredictPlot[look_back:len(trainPredict)+look_back, :] = trainPredict\n",
    "# shift test predictions for plotting\n",
    "testPredictPlot = np.empty_like(dataset)\n",
    "testPredictPlot[:, :] = np.nan\n",
    "testPredictPlot[len(trainPredict)+(look_back*2)+1:len(dataset)-1, :] = testPredict\n",
    "# plot baseline and predictions\n",
    "plt.plot(scaler.inverse_transform(dataset))\n",
    "plt.plot(trainPredictPlot)\n",
    "plt.plot(testPredictPlot)\n",
    "plt.title(\"Neural Network Model - Train and Test  data\")\n",
    "plt.xlabel('Months (in progressive order) )')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "#plt.legend(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='XG'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### 25. Forecasting  XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xg = ts_copy.copy() # copy for XGBoost modelling\n",
    "#xg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.2f}'.format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "XGBoost (Extreme Gradient Boosting) is an implementation of gradient boosting used for classification and regression problems.\n",
    "It is an ensemble of decision trees algorithm where new trees fix errors of those trees that are already part of the model. \n",
    "Trees are added until no further improvements can be made to the model.\n",
    "\n",
    "XGBoost can also be used for time series forecasting. To do so, the time series dataset needs to be \n",
    "transformed into a supervised learning problem first. This can be done by using the slide window representation \n",
    "where existing values are used as input variables and the immediate next time step as the output variable.\n",
    "\n",
    "To evaluate the model, the walk-forward validation needs to be used, where the time based nature of the data is taken \n",
    "into consideration. For this reason, k-fold cross validation cannot be used.\n",
    "'''\n",
    "# transform a time series dataset into a supervised learning dataset \n",
    "# by aligning existing values (used as input) to its next predefined no of values (used as output)\n",
    "\n",
    "# this is the preparation sample data to train XGboost model (each row correspond to 1 sample) \n",
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True): \n",
    "    \"\"\"A function to set up the ts as a df with n_in width (1 by default) \"\"\"\n",
    "    df = pd.DataFrame(data)\n",
    "    cols = list()\n",
    "    # input sequence (t-n, ... t-1)\n",
    "     #col1 = copy a fixed amount of datapoint (equal to len(df), ie 71 datapoints) from the beginning of the df, n_in times\n",
    "        # these copies are stacked in 1 column, and each copy (of this fixed length df) begins with a decreasing no of nan values \n",
    "        # these nan values are of n_in amount, to start with, and then they go down to zero at each copy of these datapoints.\n",
    "        # the rows with nans are then dropped\n",
    "    for i in range(n_in, 0, -1): \n",
    "        cols.append(df.shift(i))\n",
    "    #print('col 1st------------------------------------>',cols)\n",
    "        \n",
    "    # forecast sequence (t, t+1, ... t+n) (prepare 1 additional column only at the end of the n_in number of columns)\n",
    "    for i in range(0, n_out):\n",
    "        cols.append(df.shift(-i))\n",
    "    #print('col 2nd----------------------------------->',cols)\n",
    "        \n",
    "    # put it all together --> the new dataframe will have n_in+1 columns\n",
    "    agg = pd.concat(cols, axis=1)\n",
    "    # drop rows with NaN values (those at the beginning of the dataframe; this should be n_in number of rows)\n",
    "    if dropnan:\n",
    "        agg.dropna(inplace=True)\n",
    "    #print(agg)\n",
    "    #print ('agg.values: --------------------------->', agg.values)\n",
    "    return agg.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "    \"\"\"A function to split data in train an test\"\"\"\n",
    "    #print('data1 ----------------------->.',  data[:-n_test, :])\n",
    "    #print('data2 -------------------------->',data[-n_test:, :])\n",
    "    return data[:-n_test, :], data[-n_test:, :]\n",
    "\n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train, testX):\n",
    "    \"\"\"A function to fit XGBoost\"\"\"\n",
    "    # transform list into array\n",
    "    train = np.asarray(train)\n",
    "    # split into input and output columns\n",
    "    trainX = train[:, :-1]\n",
    "    #print('trainX--------------->',trainX)\n",
    "    trainy = train[:, -1]\n",
    "    #print('trainy--------------->',trainy)\n",
    "    # fit model\n",
    "    model = XGBRegressor(objective='reg:squarederror', n_estimators=1000)\n",
    "    model.fit(trainX, trainy)\n",
    "    # make a one-step prediction\n",
    "    yhat = model.predict(np.asarray([testX]))\n",
    "    #print('yhat-------------------------------->', yhat)\n",
    "    return yhat[0]\n",
    " \n",
    "# walk-forward validation for univariate data\n",
    "\n",
    "def walk_forward_validation(data, n_test):\n",
    "    \"\"\"A function to validate the data with walk_forward\"\"\"\n",
    "    predictions = list()\n",
    "    # split dataset\n",
    "    train, test = train_test_split(data, n_test)\n",
    "    #print('train---------------------------->',train)\n",
    "    print()\n",
    "    #print('test--------------------------->',test)\n",
    "    # seed history with training dataset\n",
    "    history = [x for x in train]\n",
    "    #print('history: ---------------------------------------------->',history)\n",
    "    # step over each time-step in the test set\n",
    "    for i in range(len(test)):\n",
    "        # split test row into input and output columns\n",
    "        testX, testy = test[i, :-1], test[i, -1]\n",
    "        #print(testX,testy)\n",
    "        # fit model on history and make a prediction\n",
    "        yhat = xgboost_forecast(history, testX)\n",
    "        # store forecast in list of predictions\n",
    "        predictions.append(yhat)\n",
    "        # add actual observation to history for the next loop\n",
    "        history.append(test[i])\n",
    "        # summarize progress\n",
    "        print('>expected=%.1f, predicted=%.1f' % (testy, yhat))\n",
    "    # estimate prediction error\n",
    "    #print('history-------------------->', history)\n",
    "    #print('predictions----->',predictions)\n",
    "    error = mean_absolute_error(test[:, -1], predictions)\n",
    "    error_percentage = mean_absolute_percentage_error(test[:, -1], predictions)\n",
    "    error_RMSE= mean_squared_error(test[:, -1], predictions)**0.5\n",
    "    error_MASE=MeanAbsoluteScaledError()\n",
    "    error_MASE=error_MASE(test[:, -1],pd.DataFrame(predictions), y_train=data[:-n_test, -1])\n",
    "\n",
    "    return error, error_percentage, error_RMSE,error_MASE, test[:, -1], predictions\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XG_values =xg.values"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#### Transformations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the time series data into supervised learning\n",
    "# keep the existing observation's value (value no 1 from the df) and add the next 6 observations; \n",
    "# then move the next observation (no 2) and add the next 6 observations and so on\n",
    "XG_data = series_to_supervised(XG_values, n_in=6)\n",
    "#XG_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# evaluate model \n",
    "MAE_XG, MAPE_XG, RMSE_XG, MASE_XG,y, yhat = walk_forward_validation(XG_data, 23)\n",
    "#print('mae',MAE_XG, 'mape',MAPE_XG, 'rsme', RSME_XG,'y', y, 'yhat', yhat)\n",
    "# print(y)'\n",
    "# print(yhat)\n",
    "print('MAE: %.3f' % MAE_XG)\n",
    "print('MAPE: %.3f' % MAPE_XG)\n",
    "print('RMSE: %.3f' % RMSE_XG)\n",
    "print('MASE: %.3f' % MASE_XG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot expected vs predicted\n",
    "plt.figure(figsize=(12,8))\n",
    "plt.plot(y, label='Actuals Monthly Movements £')\n",
    "plt.plot(yhat, label='Predicted')\n",
    "plt.legend()\n",
    "plt.title(\"XGBoost Model - Test data\")\n",
    "plt.xlabel('Last 23 Months of TS')\n",
    "plt.ylabel(\"Values £\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Although this XGBoost is performing better the RNN model, the variance is still high. Its mae is 642 against 628 of RNN and 272 of SARIMAX. It has learned the signal much better than RNN but some of the predictions seem 'lagged' from the truth. Adding additional data points should help to achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='HW'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 26. Holt-Winters model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "wh_before=ts_before_dif.copy() \n",
    "wh_before.to_csv('angry_before.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df=ts.copy()\n",
    "HW_df.index.names = ['Month']\n",
    "HW_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. SMA - Simple Moving Averages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#SMA - Simple Moving Averages\n",
    "'''\n",
    "Moving Averages and Single Exponential Smoothing does a poor job of forecasting when there \n",
    "is trend and seasonality in the data. Double and Triple exponential smoothing is best suited for this kind of timeseries data.\n",
    "\n",
    "1 - Winter’s method assumes that the time series has a level, trend and seasonal component. \n",
    "2 - Double Exponential smoothing uses a smoothing factor that addresses trend. \n",
    "3 - Triple Exponential smoothing uses a smoothing factor that addresses seasonality.\n",
    "A Holt-Winters model is defined by its three order parameters, alpha, beta, gamma. \n",
    "Alpha specifies the coefficient for the level smoothing. \n",
    "Beta specifies the coefficient for the trend smoothing. \n",
    "Gamma specifies the coefficient for the seasonal smoothing.\n",
    "'''\n",
    "#plt.figure(figsize=(12,8))\n",
    "HW_df.dropna(inplace=True)\n",
    "HW_df['6-month-SMA'] = HW_df['Values'].rolling(window=6).mean()\n",
    "HW_df['12-month-SMA'] = HW_df['Values'].rolling(window=12).mean()\n",
    "HW_df.plot(title='Simple Moving Averages',ylabel=(\"Monthly Values £\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW_df.plot(title='Simple Moving Averages',ylabel=(\"Monthly Values £\"));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df.head(12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. EWMA - Exponentially Weighted Moving Average"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# EWMA - Exponentially Weighted Moving Average\n",
    "HW_df['ewma12'] = HW_df['Values'].ewm(span=12,adjust=False).mean()\n",
    "HW_df[['Values','ewma12']].plot(title='EWMA - Exponential Weighted Moving Avg',ylabel=(\"Monthly Values £\"));\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df[['Values','ewma12']].plot(title='EWMA - Exponential Weighted Moving Avg',ylabel=(\"Monthly Values £\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Comparing SMA to EWMA\n",
    "HW_df[['Values','ewma12','6-month-SMA','12-month-SMA']].plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. SES - Single exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the span and the smoothing factor alpha\n",
    "span = 12\n",
    "alpha = 2/(span+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df.index.freq = 'MS' \n",
    "HW_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df['SES12'] = SimpleExpSmoothing(HW_df['Values']).fit(smoothing_level=alpha,optimized=False).fittedvalues.shift(-1)\n",
    "HW_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df[['Values','ewma12','SES12']].plot(title='EWMA and Holt Winters Single Exponential Smoothing', ylabel='Monthly Values £');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Holt Exponential Smoothing (HES) - Double Exponential Smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Double Exponential Smoothing\n",
    "HW_df['DES12'] = ExponentialSmoothing(HW_df['Values'],trend='add').fit().fittedvalues.shift(-1)\n",
    "HW_df[['Values','SES12','DES12']].plot(title='Holt Winters Single & Double Exponential Smoothing', ylabel='Monthly Values £');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#last 24 months\n",
    "HW_df[['Values','SES12','DES12']].iloc[:24].plot(title='Holt Winters Single & Double Exponential Smoothing Last 24 months').autoscale(axis='x',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW_df['DES12_mul'] = ExponentialSmoothing(HW_df['Values'],trend='add').fit().fittedvalues.shift(-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df[['Values','DES12']].iloc[:24].plot(title='Holt Winters Double Exponential Smoothing(Additive)').autoscale(axis='x',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Holt-Winter (WES) - triple exponential smoothing\n",
    "HW_df['TESadd12'] = ExponentialSmoothing(HW_df['Values'],trend='add',seasonal='add',seasonal_periods=12).fit().fittedvalues\n",
    "HW_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tripple exponential smoothing\n",
    "HW_df['TESadd12'] = ExponentialSmoothing(HW_df['Values'],trend='add',seasonal='add',seasonal_periods=12).fit().fittedvalues\n",
    "HW_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#HW_df['TESmul12'] = ExponentialSmoothing(HW_df['Values'],trend='add',seasonal='add',seasonal_periods=12).fit().fittedvalues\n",
    "HW_df[['Values','TESadd12']].iloc[:24].plot(title='Holt Winters Triple Exponential Smoothing Last (Additive )').autoscale(axis='x',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HW_df[['Values','TESadd12']].plot(title='Holt Winters Triple Exponential Smoothing Last (Additive )',figsize=(16,8),ylabel='Monthly Values £').autoscale(axis='x',tight=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split into train and test set\n",
    "train_HW_df = HW_df[:l_train] \n",
    "test_HW_df = HW_df[l_train:] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_HW_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fit the model on the train set \n",
    "\n",
    "fitted_model_HW = ExponentialSmoothing(train_HW_df['Values'],\n",
    "                                    trend='add',seasonal='add',seasonal_periods=12).fit()\n",
    "predictions_HW = fitted_model_HW.forecast(l_test).rename('HW Test Forecast')\n",
    "predictions_HW[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#predictions_HW"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_HW_df['Values'].plot(legend=True,label='TRAIN')\n",
    "# test_HW_df['Values'].plot(legend=True,label='TEST',figsize=(12,8))\n",
    "# plt.title('Train and Test Data');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HW_df['Values'].plot(legend=True,label='TRAIN')\n",
    "test_HW_df['Values'].plot(legend=True,label='TEST',figsize=(16,8))\n",
    "predictions_HW.plot(legend=True,label='PREDICTION')\n",
    "#plt.xlabel('Values')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.title('Train, Test and Predicted Test using Holt Winters');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(test_HW_df.loc[:,'Values'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate and print MAE\n",
    "MAE_HW=mean_absolute_error(test_HW_df.loc[:,'Values'],predictions_HW[:ts.shape[0]])\n",
    "\n",
    "# Calculate and print MAPE\n",
    "MAPE_HW=mean_absolute_percentage_error(test_HW_df.loc[:,'Values'],predictions_HW[:ts.shape[0]])\n",
    "\n",
    "# Calculate and print RMSE\n",
    "RMSE_HW=mean_squared_error(test_HW_df.loc[:,'Values'],predictions_HW[:ts.shape[0]])**0.5\n",
    "\n",
    "# Calculate and print MASE\n",
    "MASE_HW=MeanAbsoluteScaledError()\n",
    "MASE_HW=MASE_HW(test_HW_df.loc[:,'Values'],pd.DataFrame(predictions_HW[:ts.shape[0]]), y_train=test_HW_df.loc[:,'Values'])\n",
    "\n",
    "print(\"Holt-Winters' MAE = {0:.3f}\".format(MAE_HW))\n",
    "print(\"Holt-Winters' MAPE = {0:.3f}\".format(MAPE_HW))\n",
    "print(\"Holt-Winters' RMSE = {0:.3f}\".format(RMSE_HW))\n",
    "print(\"Holt-Winters' MASE = {0:.3f}\".format(MASE_HW))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_HW_df['Values'].plot(legend=True,label='TRAIN')\n",
    "test_HW_df['Values'].plot(legend=True,label='TEST',figsize=(12,8))\n",
    "predictions_HW.plot(legend=True,label='PREDICTION',xlim=['2020-01-01','2021-12-01']);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model seems to have learnt the data fairly well, by judging the charts, but it is again underfitting. The predictions seem to follow the distribution of the true data but their values are quite different. This could be due to the nature of the data which does not really show a strictly exponential nature. The performance metrics for this model, with a MAE of 695, are the worst amongst the performance metrics examined so far. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# forecast into the future"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# change mul to add\n",
    "final_model_HW = ExponentialSmoothing(HW_df['Values'],trend='add',seasonal='add',seasonal_periods=12).fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_predictions_WH = final_model_HW.forecast(steps=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_predictions_WH.plot(legend=True,label='Sales Forecast 2022-2024')\n",
    "plt.ylabel(\"Monthly Values £\")\n",
    "plt.xlabel(\"Month\")\n",
    "plt.title('Sales Forecast 2022-2024');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Darts'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 27. Darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the timeseries into train and test dataset\n",
    "'''\n",
    "arts is comparable to other models as it follows the same principles:\n",
    "\n",
    "1 - It is based and it generates a TimeSeries (containing only datetimes and the value of the observations). \n",
    "2 - it uses fit() and predict() methods with is commont to all forecasting modesl \n",
    "3 - Darts can be used with the following methods:\n",
    "'''\n",
    "darts_train=train.reset_index().rename(columns={'index':'Dates'})\n",
    "darts_test=test.reset_index().rename(columns={'index':'Dates'})\n",
    "darts_ts=ts.reset_index().rename(columns={'index':'Dates'})\n",
    "\n",
    "ds_train = TimeSeries.from_dataframe(darts_train, 'Dates', 'Values')\n",
    "ds_test = TimeSeries.from_dataframe(darts_test, 'Dates', 'Values')\n",
    "ds_ts= TimeSeries.from_dataframe(darts_ts, 'Dates', 'Values')\n",
    "len(ds_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#dir(darts.models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.utils.statistics import plot_acf, check_seasonality\n",
    "\n",
    "plot_acf(ds_train, m=12, alpha=0.05)\n",
    "for m in range(2, 25):\n",
    "    is_seasonal, period = check_seasonality(ds_train, m=m, alpha=0.05)\n",
    "    if is_seasonal:\n",
    "        print(\"There is seasonality of order {}.\".format(period))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the model and calculate the predition on the test set\n",
    "\n",
    "model_ds = ES(seasonal_periods=12)#ExponentialSmoothing(seasonal_periods=12)\n",
    "model_ds.fit(ds_train)\n",
    "prediction_darts = model_ds.predict(len(ds_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(ds_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the whole time series and the prediction\n",
    "plt.figure(figsize=(12,8))\n",
    "ds_ts.plot(label='actual')\n",
    "prediction_darts.plot(label='forecast', lw=3)\n",
    "plt.title('Darts Sales Forecast - With Exponential Smoothing')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('£')\n",
    "plt.legend()\n",
    "\n",
    "df_darts=prediction_darts.pd_dataframe(copy=True)\n",
    "#df_darts\n",
    "\n",
    "# calculate MAE\n",
    "MAE_darts=mean_absolute_error(test, df_darts)\n",
    "\n",
    "# calculate MAPE\n",
    "MAPE_darts=mean_absolute_percentage_error(test, df_darts)\n",
    "\n",
    "# calculate RMSE\n",
    "RMSE_darts=mean_squared_error(test, df_darts)**0.5\n",
    "\n",
    "# Calculate MASE\n",
    "MASE_darts=MeanAbsoluteScaledError()\n",
    "MASE_darts=MASE_darts(test, df_darts, y_train=train)\n",
    "\n",
    "\n",
    "print(\"Darts'(ES) MAE = {0:.3f}\".format(MAE_darts))\n",
    "print(\"Darts'(ES) MAPE = {0:.3f}\".format(MAPE_darts))\n",
    "print(\"Darts'(ES) RMSE = {0:.3f}\".format(RMSE_darts))\n",
    "print(\"Darts'(ES) MASE = {0:.3f}\".format(MASE_darts))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#df_darts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from darts.models import AutoARIMA\n",
    "\n",
    "model_aarima = AutoARIMA()\n",
    "model_aarima.fit(ds_train)\n",
    "prediction_aarima = model_aarima.predict(len(ds_test))\n",
    "\n",
    "# plot the whole time series and the prediction\n",
    "plt.figure(figsize=(12,8))\n",
    "ds_ts.plot(label='actual')\n",
    "prediction_aarima.plot(label='forecast', lw=3)\n",
    "plt.title('Darts Sales Forecast - With Auto-ARIMA')\n",
    "plt.xlabel('Months')\n",
    "plt.ylabel('£')\n",
    "plt.legend()\n",
    "\n",
    "df_darts_aarima=prediction_aarima.pd_dataframe(copy=True)\n",
    "#df_darts\n",
    "\n",
    "# calculate MAE\n",
    "MAE_darts_AARIMA=mean_absolute_error(test, df_darts_aarima)\n",
    "\n",
    "# calculate MAPE\n",
    "MAPE_darts_AARIMA=mean_absolute_percentage_error(test, df_darts_aarima)\n",
    "\n",
    "# calculate RMSE\n",
    "RMSE_darts_AARIMA=mean_squared_error(test, df_darts_aarima)**0.5\n",
    "\n",
    "\n",
    "# # # Calculate MASE\n",
    "# MASE_darts_aarima=MeanAbsoluteScaledError()\n",
    "# MASE_darts_aarima=MASE_darts(test, df_darts_aarima, y_train=train)\n",
    "# print(\"Dart's MASE = {0:.3f}\".format(MASE_darts_aarima))\n",
    "\n",
    "print(\"Darts'(AARIMA) MAE = {0:.3f}\".format(MAE_darts_AARIMA))\n",
    "print(\"Darts'(AARIMA) MAPE = {0:.3f}\".format(MAPE_darts_AARIMA))\n",
    "print(\"Darts'(AARIMA)RMSE = {0:.3f}\".format(RMSE_darts_AARIMA))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_darts_aarima"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='MAE'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " # ---------------------------   Metrics   ---------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 30. Models' MAE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*5,'   MAE   ','-'*5)\n",
    "\n",
    "#print MAE list showing all models' MAE\n",
    "print('MAE - AR: {0:.3f} '.format(MAE_AR))\n",
    "print('MAE - ARMA: {0:.3f} '.format(MAE_ARIMA))\n",
    "print('MAE - SARIMAX: {0:.3f} '.format(MAE_SARIMAX))\n",
    "print('MAE - PROPHET: {0:.3f} '.format(MAE_PROPHET))\n",
    "print('MAE - XGBoost: {0:.3f} '.format(MAE_XG))\n",
    "print('MAE - Neural Network: {0:.3f} '.format(MAE_NN))\n",
    "print('MAE - Holt-Winter: {0:.3f} '.format(MAE_HW))\n",
    "print('MAE - Darts: {0:.3f} '.format(MAE_darts))\n",
    "\n",
    "MAE_ALL=[MAE_AR,MAE_ARIMA,MAE_SARIMAX,MAE_PROPHET,MAE_XG,MAE_NN,MAE_HW,MAE_darts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 31. Models' MAPE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*5,'   MAPE   ','-'*5)\n",
    "\n",
    "#print MAPE list showing all models' MAPE\n",
    "print('MAPE - AR: {0:.3f} '.format(MAPE_AR))\n",
    "print('MAPE - ARMA: {0:.3f} '.format(MAPE_ARIMA))\n",
    "print('MAPE - SARIMAX: {0:.3f} '.format(MAPE_SARIMAX))\n",
    "print('MAPE - PROPHET: {0:.3f} '.format(MAPE_PROPHET))\n",
    "print('MAPE - XGBoost: {0:.3f} '.format(MAPE_XG))\n",
    "print('MAPE - Neural Network: {0:.3f} '.format(MAPE_NN))\n",
    "print('MAPE - Holt-Winter: {0:.3f} '.format(MAPE_HW))\n",
    "print('MAPE - Darts: {0:.3f} '.format(MAPE_darts))\n",
    "\n",
    "MAPE_ALL=[MAPE_AR,MAPE_ARIMA,MAPE_SARIMAX,MAPE_PROPHET,MAPE_XG,MAPE_NN,MAPE_HW,MAPE_darts]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 32. Models' RSME comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*5,'   RMSE   ','-'*5)\n",
    "\n",
    "#print RMSE list showing all models' RMSE\n",
    "print('RMSE - AR: {0:.3f} '.format(RMSE_AR))\n",
    "print('RMSE - ARMA: {0:.3f} '.format(RMSE_ARIMA))\n",
    "print('RMSE - SARIMAX: {0:.3f} '.format(RMSE_SARIMAX))\n",
    "print('RMSE - PROPHET: {0:.3f} '.format(RMSE_PROPHET))\n",
    "print('RMSE - XGBoost: {0:.3f} '.format(RMSE_XG))\n",
    "print('RMSE - Neural Network: {0:.3f} '.format(RMSE_NN))\n",
    "print('RMSE - Holt-Winter: {0:.3f} '.format(RMSE_HW))\n",
    "print('RMSE - Darts: {0:.3f} '.format(RMSE_darts))\n",
    "\n",
    "RMSE_ALL=[RMSE_AR,RMSE_ARIMA,RMSE_SARIMAX,RMSE_PROPHET,RMSE_XG,RMSE_NN,RMSE_HW,RMSE_darts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 33. Models' MASE comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('-'*5,'   MASE   ','-'*5)\n",
    "\n",
    "#print MAPE list showing all models' RSME\n",
    "print('MASE - AR: {0:.3f} '.format(MASE_AR))\n",
    "print('MASE - ARMA: {0:.3f} '.format(MASE_ARIMA))\n",
    "print('MASE - SARIMAX: {0:.3f} '.format(MASE_SARIMAX))\n",
    "print('MASE - PROPHET: {0:.3f} '.format(MASE_PROPHET))\n",
    "print('MASE - XGBoost: {0:.3f} '.format(MASE_XG))\n",
    "print('MASE - Neural Network: {0:.3f} '.format(MASE_NN))\n",
    "print('MASE - Holt-Winter: {0:.3f} '.format(MASE_HW))\n",
    "print('MASE - Darts: {0:.3f} '.format(MASE_darts))\n",
    "\n",
    "MASE_ALL=[MASE_AR,MASE_ARIMA,MASE_SARIMAX,MASE_PROPHET,MASE_XG,MASE_NN,MASE_HW,MASE_darts]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 34. Metrics Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_metrics=pd.DataFrame([MAE_ALL,MAPE_ALL,RMSE_ALL, MASE_ALL]).T\n",
    "all_metrics.rename(columns={0:'MAE',1:'MAPE',2:'RMSE',3:'MASE'}, \n",
    "                   index={0:'AR',1:'ARIMA',2:'SARIMAX',3:'PROPHET',4:'XGBOOST',5:'RNN',6:'HOLT-WINTERS',7:'DARTS'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='FOR'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bring forecasts back to original scale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 40. Monthly Forecasts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get last value of original time series \n",
    "ts_pre=ts_before_dif.rename(columns={'Values':'Original Monthly Movements'})\n",
    "last_ts_value=ts_pre.iloc[-1,0]\n",
    "last_ts_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all predictions out of sample (converted into a df)\n",
    "ts_future=future_prediction_sar.to_frame().rename(columns={'predicted_mean':'Differentiated Monthly Forecasts'})\n",
    "ts_future.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ts_future.iloc[0,0]=last_ts_value\n",
    "ts_future.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate final predictions in original values\n",
    "final_predictions=ts_future.cumsum()\n",
    "final_predictions=final_predictions.rename(columns={'Differentiated Monthly Forecasts':'Final Monthly Forecast'})\n",
    "final_predictions.iloc[0,0]=0\n",
    "final_predictions.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine all data (original time series and future forecasts)\n",
    "all_sales_info=pd.concat([ts_pre, ts_future,final_predictions],axis=1)\n",
    "all_sales_info=all_sales_info.fillna(0)\n",
    "# wrong all_sales_info['All Monthly Sales']=all_sales_info['Original Monthly Movements']+all_sales_info['Final Monthly Forecast']\n",
    "all_sales_info#.tail(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sales_info[['Original Monthly Movements','Final Monthly Forecast']].plot(figsize=(12,8), xlabel='Months', ylabel='£', title='Monthly Movements');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 41. YTD Forecast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytd values (without future forecasts)\n",
    "plt.figure(figsize=(12,8))\n",
    "df_final['Original_values'].plot(figsize=(12,8), xlabel='Months', ylabel='£', title='Actuals YTD Balances £')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YTD forecast\n",
    "ytd_forecasts=all_sales_info['Final Monthly Forecast'].cumsum().to_frame()\n",
    "ytd_forecasts=ytd_forecasts.rename(columns={'Final Monthly Forecast':'YTD Forecast'})\n",
    "ytd_forecasts.tail(14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ytd_sales_combined=pd.concat([df_final,ytd_forecasts],axis=1)\n",
    "ytd_sales_combined=ytd_sales_combined.fillna(0)\n",
    "ytd_sales_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ytd_sales_combined['Final YTD sales']=ytd_sales_combined['Original_values']+ytd_sales_combined['Final Monthly Forecast']\n",
    "\n",
    "# ytd_sales_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#plt.figure(figsize=(18,12))\n",
    "ytd_sales_combined[['Original_values','YTD Forecast']].plot(figsize=(8,6), \n",
    "                                                            xlabel='Months', ylabel='£', title='Actuals and Forecasts -  YTD Balances £');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_end = time.time()\n",
    "\n",
    "total_first_block = t_end-t_start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_first_block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Pickling the model file for deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(sarima_fit,open('salesmodel.pkl','wb'))\n",
    "pickled_model=pickle.load(open('salesmodel.pkl','rb'))\n",
    "pickled_model.predict(end=future[-1])[-len(future):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hyper'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ---------      Appendix  - Sarimax Model optimization         ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 50.  HyperParameters Optimization (with fixed train/ test split)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the data point where the train set stops and test set data start\n",
    "# set 70% as train test split for running optimization below, in line with the rest of the project coding.\n",
    "# Other % can be tested.\n",
    "\n",
    "train_test_split_per_optim = train_test_split_per \n",
    "h_l_train=int(len(ts)*train_test_split_per_optim)\n",
    "h_l_test= len(ts)-h_l_train\n",
    "\n",
    "# identify the train set and test set\n",
    "h_train =  train #ts[:-h_l_test] \n",
    "h_test =  test# ts[h_l_train:]\n",
    "\n",
    "# work out shape of train and test set and ts set\n",
    "print('Train: ',h_train.shape, 'Test: ', h_test.shape, 'Ts: ', ts.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set range of p, and q to take values from 0 to 4, to be used to generate triplets and quadruplets \n",
    "# the p and q identified by arma_order_select_ic where 2 and 3 respectively so within the range of values here considered\n",
    "p = q = range(0, 4)\n",
    "\n",
    "# Set d to take values from 0 to 1.\n",
    "d = range(0,1)\n",
    "\n",
    "# Initialise a list to store the MAE, pdq and seasonal values\n",
    "mae_values = []\n",
    "pdq_values=[]\n",
    "seasonal_values=[]\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets\n",
    "pdq = list(itertools.product(p, d, q))\n",
    "\n",
    "# Generate all different combinations of seasonal p, q and q and no 12 (referring to the monthly nature of data)\n",
    "seasonal_pdq = [(x[0], x[1], x[2], 12) for x in list(itertools.product(p, d, q))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a SARIMAX model with the selected list of parameters (in the itertool) for both pqd and the seasonal order\n",
    "for param_pdq in pdq:# Iterate over each option of pqd\n",
    "    for param_seasonal in seasonal_pdq:# Iterate over each option in seasonal_pqd        \n",
    "        mod_opt=tsa.statespace.SARIMAX(h_train, trend='c',\n",
    "                                                order=param_pdq, \n",
    "                                                seasonal_order=param_seasonal,\n",
    "                                                enforce_stationarity=False,\n",
    "                                                enforce_invertibility=False)\n",
    "\n",
    "        # Fit and predict the model\n",
    "        results=mod_opt.fit()\n",
    "        s_prediction=results.predict(end=ts.index[-1])[-len(test):] \n",
    "        \n",
    "        # calculate MAE\n",
    "        mae=mean_absolute_error(h_test, s_prediction)\n",
    "        \n",
    "       # add MAE and relative pdq values and seasonal values to 3 separate lists\n",
    "        mae_values.append(mae)\n",
    "        pdq_values.append(param_pdq)\n",
    "        seasonal_values.append(param_seasonal)\n",
    "        \n",
    "# create a df with the 3 lists mentioned above: MAE and relative pdq values and seasonal values\n",
    "S_MAE_df = pd.DataFrame(\n",
    "    {'pdq_values': pdq_values,\n",
    "     'seasonal_values': seasonal_values,\n",
    "     'mae_values': mae_values\n",
    "    })\n",
    "\n",
    "#identify pdq and seasonal values giving lower MAE\n",
    "S_MAE_df[S_MAE_df['mae_values']==S_MAE_df['mae_values'].min()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ------------------- cross validation with Sarimax ------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 51. Cross Validation with Sarimax (with fixed order and seasonal order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_train_size=int(len(ts)*.65) # set a min trainig size\n",
    "\n",
    "# Lists which will contain all the model's train size and MAE error.\n",
    "\n",
    "x_mae_errors = []\n",
    "x_size_training = []\n",
    "\n",
    "# Generate a SARIMAX model with the selected parameters for both pqd and the seasonal order and the train/ test split     \n",
    "\n",
    "for training_size in range(min_train_size,len(ts) ):# Iterate over different train/ test sizes\n",
    "    x_mod=tsa.statespace.SARIMAX(ts[:training_size], trend='c',\n",
    "                            order=(2,0,3),\n",
    "                            seasonal_order=(0,0,2,12),\n",
    "                            enforce_stationarity=False,\n",
    "                            enforce_invertibility=False)\n",
    "    train = ts[:training_size]\n",
    "    test = ts[training_size:]\n",
    "\n",
    "    # Fit and predict the model\n",
    "    x_results=x_mod.fit()\n",
    "    x_prediction=x_results.predict(end=len(ts)-1)[-len(test):]\n",
    "    \n",
    "    # compute the MAE:\n",
    "    x_mae = mean_absolute_error(ts.values[training_size:], x_prediction)\n",
    "    \n",
    "    # append MAE to relevant list\n",
    "    x_mae_errors.append(x_mae)\n",
    "    x_size_training.append(training_size)\n",
    "    \n",
    "# create a dataframe with training size and MAE values\n",
    "all = {'Training_size':x_size_training,'MAE':x_mae_errors}\n",
    "x_MAE_results = pd.DataFrame(all)\n",
    "#x_MAE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3 lowest MAE level are:')\n",
    "x_MAE_results.sort_values(by=['MAE'])\n",
    "x_MAE_results.nsmallest(3, 'MAE')\n",
    "#x_MAE_results[x_MAE_results['MAE']==x_MAE_results['MAE'].min()]\n",
    "#\"{:.7f}\".format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# --------------- Cross validation & hyper parameters  tuning -------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "t0 = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 52.  Cross Validation and HyperParameters Optimization (with variable train/ test split and variable order and seasonal order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Values to generate triplets and quadruplets \n",
    "pp = qq = range(0, 4)\n",
    "dd = range(0,1)\n",
    "\n",
    "# Generate all different combinations of p, q and q triplets and quadruplets\n",
    "cv_pdq = list(itertools.product(pp, dd, qq))\n",
    "cv_seasonal_pdq = [(xx[0], xx[1], xx[2], 12) for xx in list(itertools.product(pp, dd, qq))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='Hyperparameter search'></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lists which will contain all the model's parameters, train size and MAE error.\n",
    "''' the train-test split % is set to be between 65% to 80%'''\n",
    "cv_pdq_values=[]\n",
    "cv_seasonal_values=[]\n",
    "cv_mae_errors = []\n",
    "cv_size_training = []\n",
    "\n",
    " # Generate a SARIMAX model with the selected parameters for both pqd and the seasonal order and the train/ test split\n",
    "for cv_param_pdq in cv_pdq:# Iterate over each option of pqd\n",
    "    for cv_param_seasonal in cv_seasonal_pdq:# Iterate over each option in seasonal_pqd   \n",
    "\n",
    "        for training_size in range(min_train_size,len(ts) ):# Iterate ove each option of train/ test split\n",
    "            # choose the optimal lag using AIC (this is a model selection criterion)\n",
    "            cv_mod=tsa.statespace.SARIMAX(ts[:training_size], trend='c',\n",
    "                                    order=cv_param_pdq,\n",
    "                                    seasonal_order=cv_param_seasonal,\n",
    "                                    enforce_stationarity=False,\n",
    "                                    enforce_invertibility=False)\n",
    "            train = ts[:training_size]\n",
    "            test = ts[training_size:]\n",
    "\n",
    "            # Fit and predict the model\n",
    "            cv_results=cv_mod.fit()\n",
    "            cv_prediction=cv_results.predict(end=len(ts)-1)[-len(test):] #len(h_test))\n",
    "            \n",
    "            # Calculate MAE\n",
    "            cv_mae = mean_absolute_error(ts.values[training_size:], cv_prediction)\n",
    "            \n",
    "            # Append MAE, training size, order and seasonal orders values to lists\n",
    "            cv_mae_errors.append(cv_mae)\n",
    "            cv_size_training.append(training_size)\n",
    "            cv_pdq_values.append(cv_param_pdq)\n",
    "            cv_seasonal_values.append(cv_param_seasonal)\n",
    "            \n",
    "        #create dataframe containing all the mentioned list\n",
    "        all = {'pdq':cv_pdq_values, 'Seasonal':cv_seasonal_values ,'Training_size':cv_size_training,'MAE':cv_mae_errors}\n",
    "        CV_MAE_results = pd.DataFrame(all)\n",
    "# show results        \n",
    "CV_MAE_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CV_MAE_results.sort_values(by=['MAE'], ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('3 lowest MAE level are:')\n",
    "CV_MAE_sized=CV_MAE_results[CV_MAE_results['Training_size']<=int(ts.shape[0]*0.8)]\n",
    "CV_MAE_TOP15=CV_MAE_sized.sort_values(by=['MAE'], ascending=False)\n",
    "CV_MAE_TOP15.nsmallest(15, 'MAE')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = time.time()\n",
    "\n",
    "total = t1-t0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
